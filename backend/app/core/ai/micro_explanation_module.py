# backend/app/core/ai/micro_explanation_module.py

from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate
from langchain_core.messages import SystemMessage, AIMessage

class MicroExplanationModule:
    """
    Module responsible for generating concise micro-explanations for key terms
    or concepts when the AI detects a need for quick clarification.
    """

    def __init__(self, model_name: str = "gpt-4o", temperature: float = 0.5):
        """
        Initializes the MicroExplanationModule with an LLM and a prompt template.

        Args:
            model_name: The name of the OpenAI model to use.
            temperature: The sampling temperature for the LLM (lower for more factual/less creative).
        """
        self.llm = ChatOpenAI(model_name=model_name, temperature=temperature)
        self.prompt = ChatPromptTemplate.from_messages(
            [
                SystemMessage(
                    content=(
                        "You are an AI assistant designed to provide very concise, "
                        "one-to-two sentence micro-explanations of key terms or concepts. "
                        "Your goal is to offer a quick, clear definition or a brief summary "
                        "that helps the user understand the core idea without overwhelming them. "
                        "The explanation should be factual, to the point, and directly relevant "
                        "to the key term and inferred topic."
                    )
                ),
                HumanMessagePromptTemplate.from_template(
                    "User input: \"{user_input}\"\n"
                    "Key term to explain: \"{key_term}\"\n"
                    "Inferred topic: \"{topic}\"\n\n"
                    "Please provide a micro-explanation for '{key_term}' in the context of '{topic}'. "
                    "Example: \"In {topic}, '{key_term}' refers to [concise definition/explanation].\""
                ),
            ]
        )

    def generate_micro_explanation(self, user_input: str, inferred_context: dict) -> str:
        """
        Generates a micro-explanation using an LLM based on the key term and context.

        Args:
            user_input: The original input from the user (for context, though not directly in output).
            inferred_context: A dictionary containing inferred information,
                              such as 'key_term' and 'topic'.

        Returns:
            A concise micro-explanation generated by the LLM.
        """
        key_term = inferred_context.get("key_term")
        topic = inferred_context.get("topic")

        if not key_term or not topic:
            # Fallback if context is missing or incomplete
            return "I can provide a micro-explanation if you specify the term and topic more clearly."

        chain = self.prompt | self.llm
        response_content = chain.invoke({
            "user_input": user_input,
            "key_term": key_term,
            "topic": topic
        }).content
        return response_content

# Example of how this might be used (requires OpenAI API key to be set up)
if __name__ == '__main__':
    # Ensure OPENAI_API_KEY environment variable is set
    import os
    if "OPENAI_API_KEY" not in os.environ:
        print("Please set the OPENAI_API_KEY environment variable.")
    else:
        micro_explanation_module = MicroExplanationModule()
        user_query = "What is a neural network?"
        context = {"key_term": "neural network", "topic": "artificial intelligence"}

        print("Generating micro-explanation...")
        explanation = micro_explanation_module.generate_micro_explanation(user_query, context)
        print(f"Generated Explanation: {explanation}")

        # Example with missing context
        no_context_explanation = micro_explanation_module.generate_micro_explanation("Explain this.", {{}})
        print(f"\nNo context scenario: {no_context_explanation}")
