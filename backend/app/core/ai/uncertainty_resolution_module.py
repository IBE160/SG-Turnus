# backend/app/core/ai/calibration_module.py

import random
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate
from langchain_core.messages import SystemMessage, AIMessage

class UncertaintyResolutionModule:
    """
    Module responsible for generating calibration questions when the AI's
    confidence is low, utilizing LangChain and an LLM.
    """

    def __init__(self, model_name: str = "gpt-4o", temperature: float = 0.7):
        """
        Initializes the CalibrationModule with an LLM and a prompt template.

        Args:
            model_name: The name of the OpenAI model to use.
            temperature: The sampling temperature for the LLM.
        """
        self.llm = ChatOpenAI(model_name=model_name, temperature=temperature)
        self.prompt = ChatPromptTemplate.from_messages(
            [
                SystemMessage(
                    content=(
                        "You are an AI assistant designed to help users clarify their intentions "
                        "when the system is uncertain about their input. Your goal is to ask "
                        "a clear, concise calibration question that helps narrow down the user's "
                        "meaning without being leading or making assumptions. "
                        "The question should be polite and directly related to the user's input "
                        "and the inferred context."
                        "Avoid definitive statements and instead use exploratory phrasing or calibration questions."
                    )
                ),
                HumanMessagePromptTemplate.from_template(
                    "User input: \"{user_input}\"\n"
                    "Inferred key term: \"{key_term}\"\n"
                    "Inferred topic: \"{topic}\"\n\n"
                    "Based on this information, please generate a calibration question to clarify the user's intent. "
                    "Example: \"When you say '{key_term}', are you thinking about its {topic} aspect, or something else?\""
                ),
            ]
        )

    def generate_question(self, user_input: str, inferred_context: dict) -> str:
        """
        Generates a calibration question using an LLM based on the user input and context.

        Args:
            user_input: The original input from the user.
            inferred_context: A dictionary containing inferred information,
                              such as key terms and the topic. Expected keys:
                              'key_term' and 'topic'.

        Returns:
            A formatted calibration question generated by the LLM.
        """
        key_term = inferred_context.get("key_term")
        topic = inferred_context.get("topic")

        if not key_term or not topic:
            # Fallback if context is missing
            return "Could you please provide more context or clarify your question?"

        chain = self.prompt | self.llm
        response_content = chain.invoke({
            "user_input": user_input,
            "key_term": key_term,
            "topic": topic
        }).content
        return response_content

# Example of how this might be used (requires OpenAI API key to be set up)
if __name__ == '__main__':
    # Ensure OPENAI_API_KEY environment variable is set
    import os
    if "OPENAI_API_KEY" not in os.environ:
        print("Please set the OPENAI_API_KEY environment variable.")
    else:
        calibration_module = CalibrationModule()
        user_query = "Tell me about memory management in operating systems."
        context = {"key_term": "memory management", "topic": "operating systems"}

        print("Generating calibration question...")
        question = calibration_module.generate_question(user_query, context)
        print(f"Generated Question: {question}")
        print(f"Type of response: {type(question)}")

        # Example with missing context
        no_context_question = calibration_module.generate_question("What is it?", {})
        print(f"\nNo context scenario: {no_context_question}")
        print(f"Type of response: {type(no_context_question)}")
