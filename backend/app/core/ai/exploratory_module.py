# backend/app/core/ai/exploratory_module.py

from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate
from langchain_core.messages import SystemMessage

class ExploratoryModule:
    """
    Module responsible for generating exploratory phrasing when the AI's
    confidence is low, utilizing LangChain and an LLM.
    """

    def __init__(self, model_name: str = "gpt-4o", temperature: float = 0.7):
        """
        Initializes the ExploratoryModule with an LLM and a prompt template.

        Args:
            model_name: The name of the OpenAI model to use.
            temperature: The sampling temperature for the LLM.
        """
        self.llm = ChatOpenAI(model_name=model_name, temperature=temperature)
        self.prompt = ChatPromptTemplate.from_messages(
            [
                SystemMessage(
                    content=(
                        "You are an AI assistant designed to offer exploratory phrasing "
                        "when the system is uncertain about the user's input. Your goal is to "
                        "gently suggest possible interpretations or directions the user might be "
                        "interested in, without making definitive statements. "
                        "The phrasing should be polite, open-ended, and relevant to the user's input "
                        "and the inferred context. Avoid being prescriptive."
                    )
                ),
                HumanMessagePromptTemplate.from_template(
                    "User input: \"{user_input}\"\n"
                    "Inferred key term: \"{key_term}\"\n"
                    "Inferred topic: \"{topic}\"\n\n"
                    "Based on this information, please generate an exploratory phrasing to help the user. "
                    "Example: \"Perhaps you're looking for information on '{key_term}' within the realm of {topic}?\""
                ),
            ]
        )

    def generate_phrasing(self, user_input: str, inferred_context: dict) -> str:
        """
        Generates an exploratory phrasing using an LLM based on the user input and context.

        Args:
            user_input: The original input from the user.
            inferred_context: A dictionary containing inferred information,
                              such as key terms and the topic. Expected keys:
                              'key_term' and 'topic'.

        Returns:
            An exploratory phrasing generated by the LLM.
        """
        key_term = inferred_context.get("key_term")
        topic = inferred_context.get("topic")

        if not key_term or not topic:
            # Fallback if context is missing
            return "It seems I need a bit more information. Could you clarify what you're looking for?"

        chain = self.prompt | self.llm
        response = chain.invoke({
            "user_input": user_input,
            "key_term": key_term,
            "topic": topic
        })
        return response.content

# Example of how this might be used (requires OpenAI API key to be set up)
if __name__ == '__main__':
    # Ensure OPENAI_API_KEY environment variable is set
    import os
    if "OPENAI_API_KEY" not in os.environ:
        print("Please set the OPENAI_API_KEY environment variable.")
    else:
        exploratory_module = ExploratoryModule()
        user_query = "Help me understand difficult concepts."
        context = {"key_term": "difficult concepts", "topic": "learning strategies"}

        print("Generating exploratory phrasing...")
        phrasing = exploratory_module.generate_phrasing(user_query, context)
        print(f"Generated Phrasing: {phrasing}")

        # Example with missing context
        no_context_phrasing = exploratory_module.generate_phrasing("What is this?", {})
        print(f"\nNo context scenario: {no_context_phrasing}")
