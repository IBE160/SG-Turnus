# Story 2.4: Uncertainty Handling and Calibration

**As the AI engine,**
I want to handle situations where my interpretation confidence is low,
**So that** I can avoid definitive statements and instead use calibration questions or exploratory phrasing.

## Acceptance Criteria

**Given** a low confidence score for task intent or user state inference
**When** the system needs to provide a next step
**Then** it prefers generating calibration questions (e.g., "What do you think this term means?") or exploratory phrasing (e.g., "Perhaps you're looking for...").
**And** it avoids making definitive or potentially incorrect statements.

## Prerequisites

- Story 2.2: Task Intent Detection
- Story 2.3: User State Inference

## Technical Notes

- Covers FR4. This involves logic to evaluate confidence thresholds and select appropriate response templates.
- **Architectural Reference**: The "Novel Architectural Patterns" section in `architecture.md` (specifically "AI Orchestration Pattern: 'The Clarity Engine'") states: "Guardrails and Fallbacks: Confidence scores will be used throughout the inference and planning stages. If the system's confidence in its plan is low, it will intentionally fall back to a "safe" state, such as asking a clarifying calibration question, rather than providing a potentially incorrect or unhelpful "next step."" This directly informs the implementation.

---
## Requirements Context Summary

This story, "Uncertainty Handling and Calibration," is a critical component of Epic 2, "Core AI - Clarity Engine," which aims to deliver the core AI functionality of the "Zero-Friction Instant Clarity Engine." The overall goal of Epic 2 is to enable the system to understand user intent, infer user state, and provide the single most helpful next step.

This story directly addresses Functional Requirement FR4, ensuring that the AI provides empathetic and effective guidance by gracefully handling situations where its interpretation confidence is low. Instead of providing potentially incorrect definitive statements, the system will engage the user with calibration questions or exploratory phrasing, aligning with the "Guardrails and Fallbacks" principle outlined in the architectural design.

The implementation will build upon the outputs of Story 2.2 (Task Intent Detection) and Story 2.3 (User State Inference), utilizing the confidence scores generated by those components. The logic for evaluating confidence thresholds and selecting appropriate response templates will be central to this story's development.

## Structure Alignment and Lessons Learned

As no specific previous story learnings or unified project structure documentation were available, this story will proceed based on the overall project architecture and established patterns. Implementation should adhere to the architectural decisions outlined in `architecture.md`, particularly regarding the AI Orchestration Pattern and the use of the Python backend for AI services.

## Acceptance Criteria

### Given a low confidence score for task intent or user state inference:

- **When** the system needs to provide a next step
  - **Then** it prefers generating calibration questions (e.g., "What do you think this term means?") or exploratory phrasing (e.g., "Perhaps you're looking for...").
  - **And** it avoids making definitive or potentially incorrect statements.

## Tasks & Subtasks

### 1. Implement Confidence Threshold Evaluation Logic

- [ ] Define a configurable confidence threshold for intent detection and user state inference.
- [ ] Implement a Python function/method within the AI core services (`backend/app/core/`) to evaluate if the confidence score (from Story 2.2 and 2.3) falls below this threshold.
- [ ] Integrate this evaluation logic into the AI Orchestration Pattern's "Planner" module (`backend/app/core/planner_service.py`).
- [ ] **Testing:** Write unit tests (`backend/tests/`) using Pytest for the confidence evaluation logic, ensuring correct behavior for various confidence scores (e.g., above, below, at threshold).

### 2. Develop Calibration Question Generation Module

- [ ] Design and implement a LangChain-based module (`backend/app/core/ai/calibration_module.py`) in the Python backend capable of generating diverse calibration questions relevant to the current user input and inferred context.
- [ ] Integrate this module into the "Planner" (`backend/app/core/planner_service.py`) to be invoked when confidence is low.
- [ ] Define a set of calibration question templates or prompts for the LLM to guide question generation.
- [ ] **Testing:** Write unit tests (`backend/tests/`) using Pytest to verify that calibration questions are generated correctly, are grammatically sound, and are relevant to the simulated input context.

### 3. Develop Exploratory Phrasing Generation Module

- [ ] Design and implement a LangChain-based module (`backend/app/core/ai/exploratory_module.py`) in the Python backend for generating exploratory phrasing (e.g., "Perhaps you're looking for...", "Could it be that you mean...?").
- [ ] Integrate this module into the "Planner" (`backend/app/core/planner_service.py`) to be invoked when confidence is low, as an alternative or complement to calibration questions.
- [ ] Define a set of exploratory phrasing templates or prompts for the LLM.
- [ ] **Testing:** Write unit tests (`backend/tests/`) using Pytest to verify that exploratory phrasings are generated appropriately and align with the intention of avoiding definitive statements.

### 4. Integrate Response Selection Logic

- [ ] Modify the "Planner" module (`backend/app/core/planner_service.py`) to dynamically select between:
    - a direct "next step" (when confidence is high)
    - a calibration question (when confidence is low, and more explicit user guidance is needed)
    - an exploratory phrasing (when confidence is low, and hinting might be more appropriate)
- [ ] Ensure the selection logic adheres to the principle of "avoiding definitive or potentially incorrect statements" when confidence is low.
- [ ] **Testing:** Write integration tests (`backend/tests/`) to ensure the correct type of response (direct step, calibration, exploratory) is selected and generated based on varying confidence levels and simulated user states.

### 5. Update API Contracts and Frontend Integration

- [ ] Update API response schemas (`backend/app/api/schemas.py`) to accommodate new response types for calibration questions and exploratory phrasings, ensuring the frontend can correctly parse them.
- [ ] Document changes in the OpenAPI Specification (Swagger UI will auto-generate from FastAPI).
- [ ] (If within agent scope) Implement necessary frontend changes in `the-ai-helping-tool/frontend/services/` and `the-ai-helping-tool/frontend/components/` to correctly render these new response types.
- [ ] **Testing:** Develop E2E tests (`the-ai-helping-tool/cypress/integration/`) using Cypress to verify the end-to-end flow, ensuring the frontend correctly displays calibration questions and exploratory phrasings generated by the backend based on simulated low-confidence scenarios.

## Change Log

| Date         | Version | Description        | Author |
| :----------- | :------ | :----------------- | :----- |
| 2025-12-17   | 1.0     | Initial draft      | Gemini |
